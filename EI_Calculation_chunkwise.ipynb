{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f63ea1",
   "metadata": {},
   "source": [
    "## Expected Improvement calcualtion for large number of candidate space (74 million)\n",
    "\n",
    "#### The number of candidates are more than 74 million. Therfore to avoid any memory error, chunk wise data reading and execution has been done. We are considering 1 million as chunk size. At present we are executing sequentially however parallel computing can be done  \n",
    "### ** Better works in VS Code Editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92064d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from operator import ge\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from cbfv.composition import generate_features\n",
    "from convesrion_weightpct_atpct import wtpct_To_atpct\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "rng_seed = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c14f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_atpct(in_path, out_path):\n",
    "    '''\n",
    "    Convert weigth percent to atom percent \n",
    "    \n",
    "    Paramenters:\n",
    "    Input file path name consisting of formula in wt percent, Temperature and Target\n",
    "    Outful file path name where the atom percent data will save after conversion \n",
    "    '''\n",
    "    with open(in_path) as wtpct:\n",
    "        with open(out_path, 'w') as atpct:\n",
    "            atpct.write('formula,T,target\\n')\n",
    "            next(wtpct)\n",
    "            for l in wtpct:\n",
    "                l_str = l.split(\",\")\n",
    "                formula = wtpct_To_atpct(l_str[0])\n",
    "                atpct.write(f'{formula},{l_str[1]},{l_str[2]}')\n",
    "\n",
    "\n",
    "def generate_CBFV_features(in_path,out_path,nlines):\n",
    "    \n",
    "    '''\n",
    "    Featurization using CBFV \n",
    "    \n",
    "    Paramenters:\n",
    "    Input file path name consisting of formula in atom percent, Temperature and Target\n",
    "    Outful file path name where the featurization vector will save\n",
    "    '''\n",
    "    \n",
    "    with open(in_path,'r') as input_file:\n",
    "        length = len(input_file.readlines())\n",
    "\n",
    "    ### number of loops = length/nlines\n",
    "    no_loops = math.ceil(length/nlines)\n",
    "\n",
    "    with open(out_path, 'a') as features:\n",
    "\n",
    "        features.truncate(0)\n",
    "        header = [\"formula\", \"T\", \"target\"]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(no_loops):\n",
    "            df = pd.read_csv(in_path,nrows=nlines,skiprows=i*nlines+1, header=None)\n",
    "            df.columns=header\n",
    "            \n",
    "            X_test, y_test, formulae_test, skipped_train  = generate_features(df,elem_prop='f3_revised',drop_duplicates=False,extend_features=True)\n",
    "            \n",
    "            X_test_avg = X_test[['avg_Atomic_Radius','avg_Pauling_Electronegativity','avg_number_of_valence_electrons','avg_Cohesive_energy_ev_atom',\n",
    "            'avg_Bulk_modulus_RT_Gpa','avg_Elastic_modulus_RT_Gpa','avg_Shear_modulus_RT_Gpa','avg_Melting_point_(K)','avg_rate_shear_mod_Mpa_perK',\n",
    "            'avg_Solid_Solubility_atpct','avg_lattice_constant_A','avg_BEC_percm3','avg_Av.Valence_bond_strength_ev','avg_EngelZ_e/a','T']]\n",
    "\n",
    "            df_final= pd.concat([formulae_test,X_test_avg],axis=1)\n",
    "\n",
    "            if i == 0:\n",
    "                features.write(df_final.to_csv(index=False))\n",
    "            else:\n",
    "                features.write(df_final.to_csv(index=False, header=None))   \n",
    "                \n",
    "\n",
    "\n",
    "def model_fitting_bootstrap(X_train,y_train,n_iter=1000, seed = 20):\n",
    "    \n",
    "    '''\n",
    "    Fitting the machine learning model with X_train and y_train data by using bootstrapping\n",
    "    \n",
    "    paramenters:\n",
    "    X_train = training features\n",
    "    y_tain = training target\n",
    "    n_iter = no of bootstrap sample\n",
    "    seed = 20 is fixed to have repeatability in boot straping\n",
    "    \n",
    "    return:\n",
    "    returns trained models depending on how many bootstrap sample have been choosen \n",
    "    '''\n",
    "    \n",
    "   #UTS \n",
    "    parameters = dict(criterion='mse', max_features='auto',\n",
    "                          min_samples_leaf=6, min_samples_split=5,\n",
    "                          n_estimators=50, random_state=20)\n",
    "    \n",
    "#     \n",
    "#   # YS\n",
    "#     parameters = dict(criterion='mse', max_features='auto',\n",
    "#                           min_samples_leaf=4, min_samples_split=5,\n",
    "#                           n_estimators=200, random_state=20)\n",
    "\n",
    "    # trained model list to hold all 1000 trained model\n",
    "    trained_models = []\n",
    "    \n",
    "    # index required for bootstrapping \n",
    "    index = np.arange(X_train.shape[0]) \n",
    "    \n",
    "    # setting the random seed to ensure same result \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "\n",
    "        #sample from X_train, y_train with replacement\n",
    "        index_sampled = np.random.choice(index, size=X_train.shape[0], replace=True)\n",
    "        \n",
    "        #print(index_sampled)\n",
    "\n",
    "        X_train_sample = X_train[index_sampled,:]\n",
    "        y_train_sample = y_train[index_sampled]\n",
    "        \n",
    "        \n",
    "        # Instantiate the GBR model \n",
    "        gbr = GradientBoostingRegressor(**parameters)\n",
    "        gbr.fit(X_train_sample, y_train_sample)\n",
    "        \n",
    "        trained_models.append(gbr)\n",
    "        \n",
    "    return trained_models\n",
    "\n",
    "\n",
    "\n",
    "def model_predicting(trained_model,X_test):\n",
    "    '''\n",
    "    creating a 2d array to store prediction from all the models available in trained_model\n",
    "    parmeters:\n",
    "    trained_models and X_test \n",
    "    return:\n",
    "    mean and std for each test data \n",
    "    '''\n",
    "    \n",
    "    #shape :X_test length as no of rows and total no of models as column \n",
    "    bootstrap_preds = np.zeros([len(X_test), len(trained_model)])  \n",
    "    \n",
    "    \n",
    "    for i,model in enumerate(trained_model):\n",
    "        pred_i = model.predict(X_test)\n",
    "        #print(pred_i)\n",
    "        bootstrap_preds[:,i] = pred_i\n",
    "        \n",
    "    return(bootstrap_preds.mean(1),bootstrap_preds.std(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Expected_Improvement(mu_candidate, sigma_candidate, y_train, xi=0.01):\n",
    "\n",
    "    '''\n",
    "     Calcualting the Expected Improvement\n",
    "     More information : http://krasserm.github.io/2018/03/21/bayesian-optimization/\n",
    "     parameters: mean, std, y_train, xi\n",
    "     return :\n",
    "     Expected improvement value\n",
    "     \n",
    "    '''\n",
    "     \n",
    "    \n",
    "    mu_max = np.max(y_train) \n",
    "    # mu_max is the highest material property value in the training data which is at 24 deg C **** Important\n",
    "    # therefore the candidate search space is choosen at 24 deg C\n",
    "    \n",
    "    diff = (mu_candidate - mu_max - xi)\n",
    "    z = diff/sigma_candidate\n",
    "    ei = diff*norm.cdf(z)+sigma_candidate * norm.pdf(z)\n",
    "    ei[sigma_candidate == 0.0] = 0.\n",
    "    \n",
    "    return(ei, mu_candidate, sigma_candidate)\n",
    "\n",
    "\n",
    "\n",
    "def generate_ei(train_data_path, candidates_data_path, ei_outpath, nlines = 1000000, seed = 20):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "     Generate the file consisting of Expected Improvement value for each candidate alloy \n",
    "     parameters: \n",
    "     train data path, candidate data path, ei outpath, no of lines to read at chunk \n",
    "     return :\n",
    "     Expected improvement value\n",
    "     \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    ## reading csv having entire training data \n",
    "    df_train = pd.read_csv(train_data_path)\n",
    "    X_train_unscale, y_train, formulae_train, skipped_train  = generate_features(\n",
    "                                                                    df_train,\n",
    "                                                                    elem_prop='f3_revised',\n",
    "                                                                    drop_duplicates=False,\n",
    "                                                                    extend_features=True)\n",
    "\n",
    "    # considering only the average and temperature\n",
    "    # TODO : use regex for getting the average clumn names from X_train.columns\n",
    "    X_train_avg = X_train_unscale[['avg_Atomic_Radius','avg_Pauling_Electronegativity','avg_number_of_valence_electrons','avg_Cohesive_energy_ev_atom',\n",
    "                                   'avg_Bulk_modulus_RT_Gpa','avg_Elastic_modulus_RT_Gpa','avg_Shear_modulus_RT_Gpa','avg_Melting_point_(K)','avg_rate_shear_mod_Mpa_perK',\n",
    "                                   'avg_Solid_Solubility_atpct','avg_lattice_constant_A','avg_BEC_percm3','avg_Av.Valence_bond_strength_ev','avg_EngelZ_e/a','T']]\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train_avg)\n",
    "\n",
    "    \n",
    "    trained_models = model_fitting_bootstrap(X_train,y_train,n_iter=1000, seed = seed)\n",
    "    \n",
    "    \n",
    "    with open(candidates_data_path,'r') as input_file:\n",
    "        candiate_length = len(input_file.readlines())\n",
    "\n",
    "    ### number of loops = length/nlines\n",
    "    no_loops = math.ceil(candiate_length/nlines)\n",
    "    \n",
    "    print(\"Total candidates \",candiate_length,flush=True)\n",
    "    print(\"Number of loops \",no_loops, flush=True)\n",
    "    with open(ei_outpath, 'a') as ei_file:\n",
    "        ei_file.truncate(0)\n",
    "\n",
    "        #TODO: use tqdm to check the progress\n",
    "        for i in tqdm(np.arange(no_loops)):\n",
    "            df = pd.read_csv(candidates_data_path,nrows=nlines,skiprows=i*nlines+1, header=None)\n",
    "            formula = df.iloc[:,0]\n",
    "            df.drop(columns=df.columns[0],axis=1,inplace=True)#drop formula column\n",
    "            df.columns=np.arange(15)#TODO: remove this redundant line\n",
    "            \n",
    "            X_test = scaler.transform(df)\n",
    "            mean, std = model_predicting(trained_models, X_test)\n",
    "            ei, _, _ = Expected_Improvement(mean, std, y_train)\n",
    "\n",
    "\n",
    "            df_out = pd.DataFrame({'formula':formula, 'mean': mean, 'std':std, 'ei':ei})\n",
    "            \n",
    "            if i == 0:\n",
    "                ei_file.write(df_out.to_csv(index=False))\n",
    "                \n",
    "            else:\n",
    "                #features.write('\\n')\n",
    "                ei_file.write(df_out.to_csv(index=False, header=None))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sorting_EI(ei_file_path,out_path,nlines = 1000000):\n",
    "    \n",
    "    # It reads the number of lines in chunk and sort the first 20 alloy compositions from each chunk based on ei value and save them in a .csv file\n",
    "    # It can be regarded as interim sorting \n",
    "    '''\n",
    "    \n",
    "     The function takes the 'ei' file as input \n",
    "     Sorting EI and storing \n",
    "     parameters: ei_file_path, out_path, nlines\n",
    "     return :\n",
    "     Sorted Expected improvement value from each chunk\n",
    "     \n",
    "    '''\n",
    "\n",
    "    with open(ei_file_path,'r') as input_file:\n",
    "        length = len(input_file.readlines())\n",
    "\n",
    "    ### number of loops = length/nlines\n",
    "    no_loops = math.ceil(length/nlines)\n",
    "\n",
    "    with open(out_path, 'a') as ei_sort:\n",
    "        ei_sort.truncate(0)\n",
    "\n",
    "        #TODO: use tqdm to check the progress\n",
    "        for i in tqdm(np.arange(no_loops)):\n",
    "            df_ei = pd.read_csv(ei_file_path,nrows=nlines,skiprows=i*nlines+1, header=None)\n",
    "            df_ei.columns= ['formula','mean','std','ei']\n",
    "            df_ei_final = df_ei.sort_values('ei',ascending=False)[0:20]\n",
    "            if i == 0:\n",
    "                ei_sort.write(df_ei_final.to_csv(index=False))\n",
    "            else:\n",
    "                #features.write('\\n')\n",
    "                ei_sort.write(df_ei_final.to_csv(index=False, header=None))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sorting_mean(ei_file_path,out_path,nlines = 1000000):\n",
    "    \n",
    "    \n",
    "    # It reads the number of lines in chunk and sort the first 20 alloy compositions from each chunk based on mean value and save them in a .csv file\n",
    "    # It can be regarded as interim sorting \n",
    "    '''\n",
    "    \n",
    "     The function takes the 'ei' file as input \n",
    "     Sorting mean and storing \n",
    "     parameters: ei_file_path, out_path, nlines\n",
    "     return :\n",
    "     Sorted mean value from each chunk\n",
    "     \n",
    "    '''\n",
    "\n",
    "\n",
    "    with open(ei_file_path,'r') as input_file:\n",
    "        length = len(input_file.readlines())\n",
    "\n",
    "    ### number of loops = length/nlines\n",
    "    no_loops = math.ceil(length/nlines)\n",
    "\n",
    "    with open(out_path, 'a') as mean_sort:\n",
    "        mean_sort.truncate(0)\n",
    "\n",
    "        #TODO: use tqdm or someting to check the progress\n",
    "        for i in tqdm(np.arange(no_loops)):\n",
    "            df_mean = pd.read_csv(ei_file_path,nrows=nlines,skiprows=i*nlines+1, header=None)\n",
    "            df_mean.columns= ['formula','mean','std','ei']\n",
    "            df_mean_final = df_mean.sort_values('mean',ascending=False)[0:20]\n",
    "            if i == 0:\n",
    "                mean_sort.write(df_mean_final.to_csv(index=False))\n",
    "            else:\n",
    "                #features.write('\\n')\n",
    "                mean_sort.write(df_mean_final.to_csv(index=False, header=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc4728",
   "metadata": {},
   "source": [
    "### Read the candidate alloy data and convert weight percent to atom percent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"data/compositions_test_24degc_allcombinations_5elements.csv\"\n",
    "atpct_path = \"data/quinary/compositions_test_24degc_allcombinations_atpct_5.csv\"\n",
    "generate_atpct(in_path, atpct_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf926f0",
   "metadata": {},
   "source": [
    "### Generate features using the alloy composition in atom percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_out_path = \"data/quinary/compositions_test_24degc_allcombinations_features_5.csv\"\n",
    "generate_CBFV_features(atpct_path, features_out_path, 1000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d87044",
   "metadata": {},
   "source": [
    "### Calculate EI for each candidate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_out_path = \"data/quinary/compositions_test_24degc_allcombinations_features_5.csv\"\n",
    "train_data_path = 'data/model_input/train_ultimatestrength_all.csv'\n",
    "candidates_data_path = features_out_path\n",
    "ei_outpath = 'data/quinary/ei_UTS_5.csv'\n",
    "\n",
    "generate_ei(train_data_path,candidates_data_path,ei_outpath,nlines = 1000000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorting EI_interim\n",
    "out_path = 'data/quinary/ei_UTS_sort_intm_5'\n",
    "sorting_EI(ei_file_path=ei_outpath, out_path=out_path, nlines=1000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d32c9",
   "metadata": {},
   "source": [
    "### Saving in .csv file based on suggested candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ei = pd.read_csv(out_path)\n",
    "df_ei_sorted = df_ei.sort_values('ei',ascending=False)\n",
    "df_ei_sorted.to_csv('data/quinary/UTS_ei_sort_final_5.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
